{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29d537",
   "metadata": {},
   "source": [
    "# Netcdf cleaning example\n",
    "\n",
    "This is an example of cleaning data accessed in netcdf format and preparing it for analysis. \n",
    "\n",
    "The dataset we will use contains InSAR-derived ice velocity for 10 years over the Amundsen Sea Embayment in Antarctica. The data is downloaded from: https://nsidc.org/data/NSIDC-0545/versions/1\n",
    "\n",
    "downloaded data is `.hdr` and `.dat` files for each year, and a `.nc` for all of the years together. \n",
    "\n",
    "The `.nc` object is a dataset with dimensions x,y and data vars for each year. So for each year there are `vx`,`vy`,`err` vars. We'd like to re-organize this so that there are 3 variables (`vx`, `vy` and `err`) that exist along a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.tutorial.open_dataset('ASE_ice_velocity.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dab41-befb-4805-b8a8-cf64e1696a0e",
   "metadata": {},
   "source": [
    "Take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc9437-af19-4056-bb68-08c88de525b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa5e6b-e49b-4801-93af-fcba542b606c",
   "metadata": {},
   "source": [
    "Check the projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs['Projection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093188c2-a46f-421c-a8c5-e3b40b7f975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1916e2-589d-4b2d-ab2c-0675ffb440da",
   "metadata": {},
   "source": [
    "Currently the dimensions on the object are `ny` and `nx` but the object has no coordinates. If we look in the `data_vars` we can see there are two variables named `xaxis` and `yaxis`. Let's confirm that they match the dimensions `nx` and `ny` in length and then assign them as coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4eae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.dims['ny'])\n",
    "print(ds.dims['nx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b35954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds.yaxis.data))\n",
    "print(len(ds.xaxis.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625fe5c-fa4b-418f-8387-f31b4153e83e",
   "metadata": {},
   "source": [
    "We'll assign the `xaxis` and `yaxis` vars to be coordinates, and drop them from the `data_vars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ds.assign_coords(coords={'x': ds.xaxis, 'y': ds.yaxis}).drop_vars(['xaxis', 'yaxis'])\n",
    ").swap_dims({'ny': 'y', 'nx': 'x'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1f698-86a4-4daf-829d-c9f60a25d378",
   "metadata": {},
   "source": [
    "Now we have x and y coordinates and 30 data variables. However, the `data_vars` are really only 3 unique variables that exist along a time dimension (with a length of 10). \n",
    "We want to add a time dimension to the dataset and concatenate the data variables in each of the three groups together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab21058-3105-4cd5-a7fc-33ff10c6e439",
   "metadata": {},
   "source": [
    "Start by making a few objects that we'll use while we're re-organizing. These are: a list of all the variables in the dataset (`var_ls`), a list of the years covered by the dataset that are currently stored in variable names (`yr_ls`) and then finally lists for each variable (`vx_ls`,`vy_ls` and `err_ls`). These are all of the variables in the original dataset that correspond with that main variable group (`vx`, `vy` or `err`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ls = list(ds)\n",
    "\n",
    "yr_ls = list(set([var[-4:] for var in var_ls]))\n",
    "\n",
    "yr_ls = sorted([pd.to_datetime(year).year for year in yr_ls])\n",
    "yr_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26618522-cdb9-4ec9-b267-a96b44f96e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_ls = [var for var in var_ls if 'vx' in var]\n",
    "vy_ls = [var for var in var_ls if 'vy' in var]\n",
    "err_ls = [var for var in var_ls if 'err' in var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a73217",
   "metadata": {},
   "source": [
    "Now we are going to group the `dataset.data_vars` into `vx`,`vy`, and `err` and prepare to concatenate them along the time dimension. We will perform the same operations for all three variables but we will demonstrate the process for the first variable in several steps, before showing the operation wrapped into one command for the other two variables. There is a great explanation of this kind of step [here](https://towardsdatascience.com/pythonic-way-to-perform-statistics-across-multiple-variables-with-xarray-d0221c78e34a). At the end of this step, for `vx`, `vy` and `err` we will have a list of `xr.DataArrays` that all have a time dimension on the 0-axis. \n",
    "\n",
    "In the cell below, we make a list of the `xr.DataArrays` in the original `xr.Dataset` that correspond to that variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls = [ds[var] for var in vx_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3f37e",
   "metadata": {},
   "source": [
    "You can see that `da_vx_ls` is a `list` object with a length of 10, and each element of the list is a `xr.DataArray` corresponding to `vx` vars in the original `xr.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Object type: ', type(da_vx_ls))\n",
    "print('Ojbect length: ', len(da_vx_ls))\n",
    "da_vx_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66231382",
   "metadata": {},
   "source": [
    "next, we will add a time dimension to every element of `da_vx_ls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls = [da_vx_ls[var].expand_dims('time',axis=0) for var in range(len(da_vx_ls))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b26a0",
   "metadata": {},
   "source": [
    "Now you can see that each list element is an `xr.DataArray` as before, but that there is now a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12437590",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d9a4e",
   "metadata": {},
   "source": [
    "Assign time as a coordinate to each `xr.DataArray` in the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls = [da_vx_ls[var].assign_coords(time=[var]) for var in range(len(da_vx_ls))]\n",
    "da_vx_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f50162",
   "metadata": {},
   "source": [
    "Time is now a coordinate as well as a dimension and the coordinate value corresponds to the element-order of the list, ie. the first (0-place) element of `da_vx_ls_test` is the `xr.DataArray` containing the `vx1996` variable, and the `time` coord is 0. In the second (1-place) element, the `xr.DataArray` is called `vx2000` and the `time` coord is 1. \n",
    "\n",
    "Finally, we will rename the `xr.DataArrays` to reflect just the variable name, rather than the year, because that is now referenced in the time coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d637fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls = [da_vx_ls[var].rename('vx') for var in range(len(da_vx_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c97f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f96d66",
   "metadata": {},
   "source": [
    "Now we have a list of `xr.DataArrays` for the `vx` data variable where each `xr.DataArray` has a time dimension and coordinates along the time dimension. This list is ready to be concatenated along the time dimension. \n",
    "\n",
    "First, we will perform the same steps for the other two data variables (`vy` and `err`) before concatenating all three along the time dimension and merging into one `xr.Dataset`. For `vy` and `err`, we will combine the steps followed for `vx` into one operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c05584-c58c-408a-8b90-fa7ab249764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vy_ls = [\n",
    "    ds[var].expand_dims('time', axis=0).assign_coords(time=[var]).rename('vy') for var in vy_ls\n",
    "]\n",
    "\n",
    "da_err_ls = [\n",
    "    ds[var].expand_dims('time', axis=0).assign_coords(time=[var]).rename('err') for var in err_ls\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd744d-79e1-4b80-ab43-0c93d41903e0",
   "metadata": {},
   "source": [
    "Once we have these lists, we will concatenate them together to a single `xr.DataArray` with `x`,`y` and `time` dimensions. In the above step, when we create the time dimension we assign a stand-in for the time coordinate. In the cell below, we'll use the `yr_ls` object that we created that is a list whose elements are time-aware objects corresponding to the time coordinates (originally in the variable names). The final line in the cell below merges the three `xr.DataArray`s on the common `time` dimension that they now share, so we have a `xr.Dataset` with `x`,`y` and `time` dimensions and `vx`, `vy` and `err` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e151799-b48b-48c4-a735-b79c073ba119",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_concat = xr.concat(da_vx_ls, dim='time')\n",
    "vy_concat = xr.concat(da_vy_ls, dim='time')\n",
    "err_concat = xr.concat(da_err_ls, dim='time')\n",
    "\n",
    "vx_concat['time'] = yr_ls\n",
    "vy_concat['time'] = yr_ls\n",
    "err_concat['time'] = yr_ls\n",
    "\n",
    "ds_merge = xr.merge([vx_concat, vy_concat, err_concat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f87c1-a30e-457d-be96-ebd1066c172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7d811-0852-4069-b192-5c00a60b57d6",
   "metadata": {},
   "source": [
    "We'll add a variable that is magnitude of velocity as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710e608-ae05-468a-a6de-91c3e0bfab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge['vv'] = np.sqrt((ds_merge.vx**2) + (ds_merge.vy**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc414aad-e9eb-43d7-aaa3-1d3e8eeebbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge.vv.isel(time=0).plot(vmax=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdb920-fafd-4616-b94d-20594ec0a0fa",
   "metadata": {},
   "source": [
    "and add the `attrs` of the original object to our new object, `ds_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f7978-45c3-4b86-bcd7-4d50719f966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bde3d-b32f-43bc-95f1-09c12a0508f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ff620-752e-4da5-874e-b125cd1fec57",
   "metadata": {},
   "source": [
    "Checking against original version to make sure it's the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38061b-b6a7-4301-ba82-ddda26ed0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((ds.vx1996**2) + (ds.vy1996**2)).plot(vmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66752e5-b782-4312-81b6-282e7af9a9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-itslivetools-py",
   "language": "python",
   "name": "conda-env-itslivetools-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
