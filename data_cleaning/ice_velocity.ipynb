{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29d537",
   "metadata": {},
   "source": [
    "# Netcdf cleaning example\n",
    "\n",
    "This is an example of cleaning data accessed in netcdf format and preparing it for analysis. \n",
    "\n",
    "The dataset we will use contains InSAR-derived ice velocity for 10 years over the Amundsen Sea Embayment in Antarctica. The data is downloaded from: https://nsidc.org/data/NSIDC-0545/versions/1\n",
    "\n",
    "downloaded data is `.hdr` and `.dat` files for each year, and a `.nc` for all of the years together. \n",
    "\n",
    "The `.nc` object is a dataset with dimensions x,y and data vars for each year. So for each year there are `vx`,`vy`,`err` vars. We'd like to re-organize this so that there are 3 variables (`vx`, `vy` and `err`) that exist along a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86663713-32d9-4c57-9656-df23c7fae2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.tutorial.open_dataset('ASE_ice_velocity.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dab41-befb-4805-b8a8-cf64e1696a0e",
   "metadata": {},
   "source": [
    "Take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc9437-af19-4056-bb68-08c88de525b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa5e6b-e49b-4801-93af-fcba542b606c",
   "metadata": {},
   "source": [
    "Check the projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs['Projection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093188c2-a46f-421c-a8c5-e3b40b7f975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1916e2-589d-4b2d-ab2c-0675ffb440da",
   "metadata": {},
   "source": [
    "Currently the dimensions on the object are `ny` and `nx` but the object has no coordinates. If we look in the `data_vars` we can see there are two variables named `xaxis` and `yaxis`. Let's confirm that they match the dimensions `nx` and `ny` in length and then assign them as coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4eae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dims['ny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b35954",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds.yaxis.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625fe5c-fa4b-418f-8387-f31b4153e83e",
   "metadata": {},
   "source": [
    "We'll assign the `xaxis` and `yaxis` vars to be coordinates, and drop them from the `data_vars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ds.assign_coords(coords={'x': ds.xaxis, 'y': ds.yaxis}).drop_vars(['xaxis', 'yaxis'])\n",
    ").swap_dims({'ny': 'y', 'nx': 'x'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1f698-86a4-4daf-829d-c9f60a25d378",
   "metadata": {},
   "source": [
    "Now we have x and y coordinates and 30 data variables. However, the `data_vars` are really only 3 unique variables that exist along a time dimension (with a length of 10). \n",
    "We want to add a time dimension to the dataset and concatenate the data variables in each of the three groups together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab21058-3105-4cd5-a7fc-33ff10c6e439",
   "metadata": {},
   "source": [
    "Start by making a few objects that we'll use while we're re-organizing. These are: a list of all the variables in the dataset (`var_ls`), a list of the years covered by the dataset that are currently stored in variable names (`yr_ls`) and then finally lists for each variable (`vx_ls`,`vy_ls` and `err_ls`). These are all of the variables in the original dataset that correspond with that main variable group (vx, vy or err)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ls = list(ds)\n",
    "\n",
    "yr_ls = list(set([var[-4:] for var in var_ls]))\n",
    "\n",
    "yr_ls = sorted([pd.to_datetime(year).year for year in yr_ls])\n",
    "yr_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26618522-cdb9-4ec9-b267-a96b44f96e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_ls = [var for var in var_ls if 'vx' in var]\n",
    "vy_ls = [var for var in var_ls if 'vy' in var]\n",
    "err_ls = [var for var in var_ls if 'err' in var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c215efc-791e-4c8f-84fc-4b0139a06b59",
   "metadata": {},
   "source": [
    "Now we are going to group the `dataset.data_vars` into `vx`, `vy` and `err` and prepare to concatenate them along the time dimension. \n",
    "In the cell below, for each variable, we are making a list of the `xr.DataArray` in the original `xr.Dataset` that corresponds to that variable. But, within that command we are also creating a new dimension (`time`) that we will use to concatenate the variables. There is a great explanation of this kind of step [here](https://towardsdatascience.com/pythonic-way-to-perform-statistics-across-multiple-variables-with-xarray-d0221c78e34a). At the end of this step, for `vx`, `vy` and `err` we will have a list of `xr.DataArrays` that all have a time dimension on the 0-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c05584-c58c-408a-8b90-fa7ab249764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_vx_ls = [\n",
    "    ds[var].expand_dims('time', axis=0).assign_coords(time=[var]).rename('vx') for var in vx_ls\n",
    "]\n",
    "\n",
    "da_vy_ls = [\n",
    "    ds[var].expand_dims('time', axis=0).assign_coords(time=[var]).rename('vy') for var in vy_ls\n",
    "]\n",
    "\n",
    "da_err_ls = [\n",
    "    ds[var].expand_dims('time', axis=0).assign_coords(time=[var]).rename('err') for var in err_ls\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd744d-79e1-4b80-ab43-0c93d41903e0",
   "metadata": {},
   "source": [
    "Once we have these lists, we will concatenate them together to a single `xr.DataArray` with `x`,`y` and `time` dimensions. In the above step, when we create the time dimension we assign a stand-in for the time coordinate. In the cell below, we'll use the `yr_ls` object that we created that is a list whose elements are time-aware objects correspondign to the time coordinates (originally in the variable names). The final line in the cell below merges the three `xr.DataArray`s on the common `time` dimension that they now share, so we have a `xr.Dataset` with `x`,`y` and `time` dimensions and `vx`, `vy` and `err` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e151799-b48b-48c4-a735-b79c073ba119",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_concat = xr.concat(da_vx_ls, dim='time')\n",
    "vy_concat = xr.concat(da_vy_ls, dim='time')\n",
    "err_concat = xr.concat(da_err_ls, dim='time')\n",
    "\n",
    "vx_concat['time'] = yr_ls\n",
    "vy_concat['time'] = yr_ls\n",
    "err_concat['time'] = yr_ls\n",
    "\n",
    "ds_merge = xr.merge([vx_concat, vy_concat, err_concat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f87c1-a30e-457d-be96-ebd1066c172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7d811-0852-4069-b192-5c00a60b57d6",
   "metadata": {},
   "source": [
    "We'll add a variable that is magnitude of velocity as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710e608-ae05-468a-a6de-91c3e0bfab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge['speed'] = np.sqrt((ds_merge.vx**2) + (ds_merge.vy**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc414aad-e9eb-43d7-aaa3-1d3e8eeebbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge.speed.isel(time=0).plot(vmax=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdb920-fafd-4616-b94d-20594ec0a0fa",
   "metadata": {},
   "source": [
    "and add the `attrs` of the original object to our new object, `ds_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f7978-45c3-4b86-bcd7-4d50719f966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bde3d-b32f-43bc-95f1-09c12a0508f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ff620-752e-4da5-874e-b125cd1fec57",
   "metadata": {},
   "source": [
    "Checking against original version to make sure it's the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38061b-b6a7-4301-ba82-ddda26ed0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((ds.vx1996**2) + (ds.vy1996**2)).plot(vmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66752e5-b782-4312-81b6-282e7af9a9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
